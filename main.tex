\documentclass[10pt, aspectratio=1610]{beamer}

% Theming
\usetheme{Madrid}
\usecolortheme{beaver}
\usefonttheme{serif}
\setbeamerfont{frametitle}{shape=\scshape}

\usepackage{amsmath,amssymb,amsthm,mathtools}
\mathtoolsset{showonlyrefs}

\usepackage[ruled]{algorithm2e}
\usepackage{setspace}
% Overlays shouldn't mess with algorithm numbers.
\resetcounteronoverlays{algocf}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\given}{\,|\,}
\newcommand{\dd}{\mathrm{d}}
\newtheorem{proposition}[theorem]{Proposition}

\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}

% Natbib for citations.
\usepackage{natbib}
\bibliographystyle{unsrtnat}

\title{Policy Optimization with Markovian Score Climbing}
\date{February 16, 2024}
\author{Sahel Iqbal}
\institute{Aalto University, Finland}

\makeatletter
\setbeamertemplate{footline}{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.35\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
    \usebeamerfont{author in head/foot}\insertshortauthor\expandafter\ifblank\expandafter{\beamer@shortinstitute}{}{~~(\insertshortinstitute)}
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.4\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.25\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}%
    \usebeamerfont{date in head/foot}\insertshortdate{}\hspace*{2em}
    \insertframenumber{} / \inserttotalframenumber\hspace*{2ex} 
  \end{beamercolorbox}}%
  \vskip0pt%
}
\makeatother

\begin{document}
  \beamertemplatenavigationsymbolsempty
  \maketitle

  \AtBeginSection[] {
    \begin{frame}{Table of Contents}
		\tableofcontents[currentsection]
	\end{frame}
  }
  
  \begin{frame}{Works Covered}
    \begin{enumerate}
      \item \emph{Risk-Sensitive Stochastic Optimal Control as Rao-Blackwellized Markovian Score Climbing}. Hany Abdulsamad, Sahel Iqbal, Adrien Corenflos, Simo S\"arkk\"a. 2023. arXiv preprint arXiv:2312.14000.
      \item \emph{Nesting Particle Filters for Experimental Design in Dynamical Systems}. Sahel Iqbal, Adrien Corenflos, Simo S\"arkk\"a, Hany Abdulsamad. 2024. arXiv preprint arXiv:2402.07868.
    \end{enumerate}
  \end{frame}

  \section{Stochastic Optimal Control as Inference}
    \begin{frame}{Decision Making Under Uncertainty}
      Consider the discrete-time sequential decision making problem
      \begin{equation}\label{eq:soc_objective}
        \mathcal{J}(\pi) = \mathbb{E}_{x_{0:T}, u_{0:T}} \left[\sum_{t=0}^{T} c(x_t, u_t)\right],
      \end{equation}
      %
      where
      \begin{itemize}
        \item $x_t \in \mathbb{R}^m$ denote the states.
        \item $u_t \in \mathbb{R}^n$ denote the actions.
        \item $c(x_t, u_t) \geq 0$ is the stage cost.
      \end{itemize}
  
      \vskip 1cm
      The joint distribution of states and actions is
      \begin{equation}
          p(x_{0:T}, u_{0:T}) = \mu(x_{0}) \prod_{t=0}^{T-1} f(x_{t+1} \mid x_t, u_t) \, \prod_{t=0}^T \pi(u_t \mid x_t).
      \end{equation}
    \end{frame}

    \begin{frame}{Decision Making Under Uncertainty}
        For a policy space $\Pi$, we want to find
        \begin{equation*}
            \pi^{*} = \argmin_{\pi \in \Pi} \medspace \mathcal{J}(\pi).
        \end{equation*}
        ~ \\
        For a parametric form $\pi(\cdot \given x_t) \equiv \pi_{\phi}(\cdot \given x_t)$ with $\phi \in \Phi \subseteq \mathbb{R}^{l}$
        \begin{equation*}
            \phi^{*} = \argmin_{\phi \in \Phi} \medspace \mathcal{J}(\phi).
        \end{equation*}
    \end{frame}

    \begin{frame}{Some Notation}
      \begin{itemize}[<+->]
        \item Let us define an augmented state
          \begin{equation}
            z_t \coloneq (x_t, u_t).
          \end{equation}
        \item Additionally, let us define
          \begin{equation}
            \mu_{\phi}(z_0) \coloneqq \mu(x_0) \, \pi_{\phi}(u_0 \mid x_0), \quad f_{\phi}(z_{t+1} \mid z_t) \coloneqq f(x_{t+1} \mid x_t, u_t) \, \pi_{\phi}(u_{t+1} \mid x_{t+1}).
          \end{equation}
        \item The distribution of an augmented state trajectory is then
          \begin{equation}
            p_\phi(z_{0:T}) = \mu_\phi(x_0) \prod_{t=1}^T f_{\phi}(z_{t+1} \mid z_t).
          \end{equation}
      \end{itemize}
    \end{frame}

    \begin{frame}{The Pseudo-Observation}
      \begin{itemize}[<+->]
        \item Following \citet{toussaint2006probabilistic} and \citet{rawlik2013probabilistic}, let us introduce a binary random variable $y_t$ with likelihood
          \begin{equation}
            g(y_t = 1 \mid z_t) = \exp \big\{-\eta \, c(z_t)\big\},
          \end{equation}
          %
          where $\eta \in \mathbb{R}_{>0}$.
        \item This yields a joint density
          \begin{equation}
            p_\phi(z_{0:T}, y_{0:T} = 1) = p_\phi(z_{0:T}) \exp\left\{-\eta \sum_{t=0}^T c(z_t)\right\}.
          \end{equation}
        \item The likelihood can be interpreted as the \emph{probability of being optimal}.
      \end{itemize}
    \end{frame}

    \begin{frame}{The Pseudo-Observation}
      \begin{columns}
        \begin{column}{0.65\textwidth}
          \begin{itemize}
            \item<1-> We can treat this as a pseudo-observation in a state-space model.
              \begin{equation}\label{eq:paper_1_ssm}
                p_{\phi}(z_{0:T}, y_{0:T}) = \mu_{\phi}(z_0) \prod_{t=0}^{T-1} f_{\phi}(z_{t+1} \mid z_t) \prod_{t=0}^T g(y_t \mid z_t).
              \end{equation}
            \item<2-> Let us define
              \begin{equation}
                \ell(\phi) \coloneq \log p_\phi(y_{0:T} = 1) = \log \mathbb{E}_{p_\phi(z_{0:T})} \bigl[ p(y_{0:T} = 1 \mid z_{0:T}) \bigr]
              \end{equation}
          \end{itemize}
        \end{column}
        \begin{column}<1->{0.35\textwidth}
          \begin{figure}[htbp]
            \centering
            \input{figures/dgm}
            \caption{The directed graphical model underlying the control problem.}
          \end{figure}
        \end{column}
      \end{columns}
    \end{frame}

    \begin{frame}{The Marginal Likelihood}
      \begin{itemize}[<+->]
        \item Maximizing the marginal likelihood is equivalent to minimizing a risk-sensitive objective
          \begin{align}\label{eq:max_likelihood}
            \argmax_{\phi \in \Phi} \medspace \ell(\phi) &= \argmin_{\phi \in \Phi} \medspace -\frac{1}{\eta}\log \mathbb{E}_{p_{\phi}(z_{0:T})} \left[ \exp \big\{-\eta \textstyle \sum_{t=0}^{T} c(x_{t}, u_{t}) \big\} \right] \\
            &\coloneq \argmin_{\phi \in \Phi} \mathcal{J}_\eta(\phi)
          \end{align}
        \item Taylor expansion around $\eta = 0$ yields
          \begin{equation}
            \mathcal{J}_\eta(\phi) = \mathbb{E} \Bigg[ \sum_{t=0}^{T} c_{t}(x_{t}, u_{t}) \Bigg] - \frac{\eta}{2} \, \mathbb{V} \Bigg[ \sum_{t=0}^{T} c_{t}(x_{t}, u_{t}) \Bigg] + \mathcal{O}(\eta^{2}).
          \end{equation}
          Thus, this log-marginal objective amounts to a risk-sensitive variant of the control objective modulated by the temperature $\eta$.
      \end{itemize}
    \end{frame}

    \begin{frame}{Complications}
      \begin{equation}
        p_{\phi}(z_{0:T}, y_{0:T}) = \mu_{\phi}(z_0) \prod_{t=0}^{T-1} f_{\phi}(z_{t+1} \mid z_t) \prod_{t=0}^T g(y_t \mid z_t).
      \end{equation}
      \begin{itemize}
        \item In general, the SSM can be
        \begin{itemize}
          \item non-linear,
          \item non-Gaussian,
          \item bounded support.
        \end{itemize}
      \item Gaussian approximations yield biased estimates of the marginal likelihood.
      \item The solution - particle methods!
      \end{itemize}
    \end{frame}

  \section{Markovian Score Climbing for MLE}
    \begin{frame}{Preliminaries - Particle Filters}
      \begin{columns}
        \begin{column}{0.5\textwidth}
          \begin{itemize}
            \item<1-> A \emph{particle filter} is a sequential Monte Carlo algorithm that can be used to estimate:
              \begin{itemize}
                \item<2-> the pathwise filtering distributions $p_\phi(z_{0:t} \mid y_{0:t})$,
                \item<3-> the marginal likelihoods $p_\phi(y_t \mid y_{0:t-1})$.
              \end{itemize}
          \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
          \only<3>{
          \begin{algorithm}[H]
            \caption{Bootstrap particle filter.}
            \setstretch{1.35}
            \SetAlgoLined
            \LinesNumbered
            \DontPrintSemicolon
            \For{$n \gets 1, \dots, N$}{
              Sample $z_0^n \sim \mu_\phi(\cdot)$ and $w_0^n = g(y_0 \given z_0^n)$. \;
              }
              \For{$t \gets 1, \dots, T$}{
                \For{$n \gets 1, \dots, N$}{
                  \tcp{Resample}
                  Sample $A_t^n$ with $\mathbb{P}(A_t^n = k) \propto w_{t-1}^k$. \;
                  \tcp{Propagate}
                  Sample $z_t^n \sim f_\phi(\cdot \given z_{t-1}^{A_t^n})$. \;
                  \tcp{Reweight}
                  Set $w_t^n = g(y_t \given z_t^n)$.\;
                }
              }
          \end{algorithm}
          }
        \end{column}
      \end{columns}
    \end{frame}

    \begin{frame}{Preliminaries - Particle Filters}
      \begin{itemize}[<+->]
        \item Particle filters can provide unbiased estimates of the marginal likelihood for general SSMs, but obtaining its gradient is usually not possible.
        \item In literature this difficulty is often circumvented using Fisher's identity:
          \begin{equation}
            \nabla_\phi \, \ell(\phi) = \int \nabla_\phi \log p_{\phi}(z_{0:T}, y_{0:T}) \, p_\phi(z_{0:T} \given y_{0:T}) \, \dd z_{0:T}.
          \end{equation}
          $\nabla_\phi \, \ell(\phi)$ is known as the \emph{score function}.
        \item We can form a Monte Carlo estimate of the above integral using samples from the smoothing distribution, $p_\phi(z_{0:T} \given y_{0:T})$.
      \end{itemize} 
    \end{frame}

    \begin{frame}{Preliminaries - Gradient Ascent Using Particle Smoothing}
        We now have the building blocks in place to perform maximum likelihood estimation through a gradient ascent procedure:
        \begin{equation}
            \phi_k = \phi_{k-1} + \gamma_k \, \widehat{\nabla_\phi \ell}(\phi_{k-1}),
        \end{equation}
        where
        \begin{itemize}
          \item<2-> $\widehat{\nabla_\phi \ell}(\phi_{k-1})$ is a stochastic estimate of the score function.
          \item<3-> $\gamma_k$ are step sizes that satisfy $\sum_{k=1}^\infty \gamma_k = \infty, \quad \sum_{k=1}^\infty \gamma_k^2 < \infty$.
        \end{itemize}
    \end{frame}

    \begin{frame}{Preliminaries - Gradient Ascent Using Particle Smoothing}
      \begin{itemize}[<+->]
        \item However, there's one more necessary criterion for the convergence of the stochastic gradient descent (ascent) algorithm~\citep{robbins1951stochastic}, namely
          \begin{equation}
            \mathbb{E}\left[\widehat{\nabla_\phi \ell}(\phi_{k-1})\right] = \nabla_\phi \ell(\phi_{k-1})
          \end{equation}
        \item Estimates of $\nabla_\phi \, \ell(\phi)$ obtained via particle smoothing are \emph{biased for a finite number of particles $N$}.
        \item Might work empirically, but no theoretical guarantees.
      \end{itemize}
    \end{frame}

    \begin{frame}{Markovian Score Climbing For Likelihood Maximization}
      \begin{itemize}[<+->]
        \item Assume we have access to Markov chain Monte Carlo kernel $\mathcal{K}$ that is $p_\phi(z_{0:T} \given y_{0:T})$-ergodic. i.e., samples generated using repeated application of the kernel, $$Z^n \sim \mathcal{K}(\cdot \given \mathcal{K}(\cdot \given \ldots, \theta), \theta) \eqqcolon \mathcal{K}^n(\cdot \given z_{0:T}, \theta)$$ starting from an arbitrary initial trajectory asymptotically produces samples from $p_\theta(z_{0:T} \given y_{0:T})$.
        \item Then the Markovian score climbing algorithm~\citep{gu1998stochastic, naesseth2020markovian} is guaranteed to converge to a local optima of the marginal likelihood.
        \end{itemize}
    \end{frame}

    \begin{frame}{Markovian Score Climbing For Likelihood Maximization}
      \begin{center}
        \begin{minipage}{0.75\textwidth}
          \begin{algorithm}[H]
            \caption{Markovian score climbing.}
            \setstretch{1.35}
            \SetAlgoLined
            \LinesNumbered
            \DontPrintSemicolon
            \KwIn{Initial trajectory $z^{0}_{0:T}$, initial parameters $\phi_0$, number of iterations $M$, step sizes $\gamma_{1:M}$, Markov kernel $\mathcal{K}$.}
            \KwOut{$\phi^* \approx \phi_{\text{MLE}}$}
            \For{$k \gets 1,\dots, M$}{
                Sample $z^{k}_{0:T} \sim \mathcal{K}(\cdot \given z^{k-1}_{0:T}, \phi_{k-1})$ \;
                Compute $\widehat{\nabla_\phi \ell}(\phi_{k-1}) \gets \nabla_\phi \log p_\phi(z^{k}_{0:T}, y_{0:T}) \vert_{\phi=\phi_{k-1}}$ \;
                Update $\phi_k \gets \phi_{k-1} + \gamma_k \widehat{\nabla_\phi \ell}(\phi_{k-1})$\;
            }
            \Return{$\phi_M$}
          \end{algorithm}
        \end{minipage}
      \end{center}
    \end{frame}

    \begin{frame}{The MCMC Kernel}
      \begin{itemize}[<+->]
        \item The only question that remains - how do we construct the Markov kernel $\mathcal{K}$?
        \item We shall use the conditional sequential Monte Carlo kernel~\citep[CSMC,][]{andrieu2010particle}.
        \item It is implemented as a simple modification to the particle smoother.
      \end{itemize}
    \end{frame}

    \begin{frame}{Conditional SMC}
      \begin{center}
        \begin{minipage}{0.6\textwidth}
          \begin{algorithm}[H]
            \caption{Conditional SMC}
            \setstretch{1.35}
            \SetAlgoLined
            \LinesNumbered
            \DontPrintSemicolon
            \KwIn{\textcolor{blue}{Reference trajectory $z_{0:T}$}}
            \tcp{Forward filtering}
            \textcolor{blue}{Set $z_0^1 = z_0$ and $w_0^1 = g(y_0 \given z_0)$} \;
            \For{$n \gets 2, \dots, N$}{
            Sample $z_0^n \sim \mu_\phi(\cdot)$ and $w_0^n = g(y_0 \given z_0^n)$ \;
            }
            \For{$t \gets 1, \dots, T$}{
              \textcolor{blue}{Set $z_t^1 = z_t$ and $w_t^1 = g(y_t \given z_t)$} \;
              \For{$n \gets 2, \dots, N$}{
                  Sample $A_t^n$ with $\mathbb{P}(A_t^n = k) \propto w_{t-1}^k$ \;
                  Sample $z_t^n \sim f_\phi(\cdot \given z_{t-1}^{A_t^n})$ and $w_t^n = g(y_t \given z_t^n)$\;
              }
            }
          \end{algorithm}
        \end{minipage}
      \end{center}
    \end{frame}

    \begin{frame}{Conditional SMC}
      \begin{itemize}
        \setlength\itemsep{1em}
        \item The CSMC kernel is a bit wasteful as it generates $T \times N$ particles and only keeps $T$ out of them.
        \item We can reuse all the generated particles to reduce the variance of the score estimator.
      \end{itemize}
    \end{frame}

    \begin{frame}{Rao-Blackwellized Markovian Score Climbing}
      \begin{center}
        \begin{minipage}{0.75\textwidth}
          \begin{algorithm}[H]
            \caption{Rao-Blackwellized MSC}
            \setstretch{1.5}
            \SetAlgoLined
            \LinesNumbered
            \DontPrintSemicolon
            \KwIn{Trajectory $z_{0:T}$}
            \KwOut{Score estimate $\widehat{\nabla_\phi \ell}(\phi)$}
            Run forward filtering to obtain the filtered particles, their weights, and the resampling indices \;
            Backward-Sample $z^{n}_{0:T} \sim \mathcal{B}$ for all $n = 1, \dots, N$ \;
            Compute $\widehat{\nabla_\phi \ell}(\phi) \gets \frac{1}{N} \sum_{n=1}^N \nabla_\phi \log p_\phi(z^{n}_{0:T}, y_{0:T})$ \;
            \Return{$\widehat{\nabla_\phi \ell}(\phi)$}
          \end{algorithm}
        \end{minipage}
      \end{center}
    \end{frame}

    \begin{frame}{Numerical Validation}
      \begin{columns}
        \begin{column}{0.5\textwidth}
          \begin{figure}[htbp]
            \centering
            \include{figures/pendulum}
          \end{figure}
        \end{column}
        \begin{column}{0.5\textwidth}
          \begin{figure}[htbp]
            \centering
            \include{figures/cartpole}
          \end{figure}
        \end{column}
      \end{columns}
    \end{frame}

    \begin{frame}{Numerical Validation}
      \begin{columns}
        \begin{column}{0.5\textwidth}
          \begin{figure}[htbp]
            \centering
            \include{figures/double_pendulum}
          \end{figure}
        \end{column}
        \begin{column}{0.5\textwidth}
          \begin{figure}[htbp]
            \centering
            \includegraphics[scale=0.1]{figures/psoc_qr.png}
            \caption{\url{github.com/hanyas/psoc.git}}
          \end{figure}
        \end{column}
      \end{columns}
    \end{frame}

    \begin{frame}{Summary}
      \textbf{Strengths}:\vspace{1em}
      \begin{itemize}
        \setlength\itemsep{1em}
        \item Fully inference-centric approach to stochastic optimal control.
        \item State-of-the-art MCMC algorithm that avoids approximations.
        \item Theoretical guarantees backed by empirical validation.
      \end{itemize}
      \vspace{1em}
      \textbf{Limitations}:\vspace{1em}
      \begin{itemize}
        \setlength\itemsep{1em}
        \item Principled way to choose the temperature.
        \item Full access to the transition density.
      \end{itemize}
    \end{frame}

    \section{Bayesian Experimental Design}
    \begin{frame}{Bayesian Experimental Design}
      Let
      \begin{itemize}
        \item $\theta \in \Theta$ be a set of unknown parameters of interest with prior $p(\theta)$.
        \item $u \in u$ be a user controllable \emph{design}.
        \item $p(x \mid u, \theta)$ be a likelihood function.
      \end{itemize}
      
      \pause
      \begin{definition}
        The \emph{information gain}~\citep[IG,][]{lindley1956measure}, in the parameter $\theta$, on applying the design $u$ and observing the outcome $x$ is defined as
        \begin{equation}\label{eq:ig_single_experiment}
          \mathcal{G}(x, u) \coloneq \mathbb{H}[p(\theta)] - \mathbb{H}[p(\theta\mid x, u)].
        \end{equation}
      \end{definition}

      \pause
      \begin{definition}
        The \emph{expected information gain}~(EIG) is defined as
        \begin{equation}
          \mathcal{I}(u) \coloneq \mathbb{E}_{p(x \mid u)} \, \mathcal{G}(x, u).
        \end{equation}
      \end{definition}
    \end{frame}

    \begin{frame}{Sequential Bayesian Experimental Design}
      Let us define $z_0 \coloneq \{x_0\}$ and $z_{t} \coloneq \{x_t, u_{t-1}\}$ for all $t \geq 1$, and denote the outcome-design history up to time $t$ by $z_{0:t} \coloneq \{x_{0:t}, u_{0:t-1}\}$. The system dynamics is specified by
      \begin{gather}
        x_0 \sim p(x_0), \\
        u_{t-1} \sim \pi_\phi(u_{t-1} \mid z_{0:t-1}), \\
        x_t \sim f(x_t \mid x_{t-1}, u_{t-1}, \theta),
      \end{gather}
      \pause

      \vspace{0.2cm}
      The joint density of states and designs is hence
      \begin{align}\label{eq:joint_density}
        p_{\phi}(z_{0:T} \mid \theta) &= p(z_{0}) \prod_{t=1}^T p_{\phi}(z_{t} \mid z_{0:t-1}, \theta) \\
        &\coloneq p(x_0) \prod_{t=1}^T f(x_t \mid x_{t-1}, u_{t-1}, \theta) \, \pi_\phi(u_{t-1} \mid z_{0:t-1})
      \end{align}
    \end{frame}

    \begin{frame}{Sequential Bayesian Experimental Design}
      \begin{itemize}
        \item The EIG for the sequential problem is
          \begin{equation}
            \mathcal{I}(\phi) \coloneq \mathbb{E}_{p_{\phi}(z_{0:T})} \Bigl[ \mathbb{H}\bigl[p(\theta)\bigr] - \mathbb{H}\bigl[p(\theta \mid z_{0:T})\bigr] \Bigr].
          \end{equation}
        \item Our goal is to find optimal policy parameters $\phi^*$ such that
          \begin{equation}
            \phi^* \coloneq \argmax_{\phi \in \Phi} \,\, \mathcal{I}(\phi)
          \end{equation}
      \end{itemize}
    \end{frame}

    \begin{frame}{Sequential Bayesian Experimental Design}
      \begin{proposition}
        For models with additive, constant noise in the dynamics, the EIG can be written as
        \begin{equation}\label{eq:eig_constant_noise}
            \mathcal{I}(\phi) \equiv \mathbb{E}_{p_{\phi}(z_{0:T})} \left[ \sum_{t=1}^T r_{t}(z_{0:t}) \right],
        \end{equation}
        where
        \begin{gather}
          r_{t}(z_{0:t}) = - \log \int p(\theta \mid z_{0:t-1}) \, f(x_t \mid x_{t-1}, u_{t-1}, \theta) \, \dd \theta, \\
          p_\phi(z_{0:T}) = \int p_\phi(z_{0:T} \mid \theta) \, p(\theta) \, \dd \theta.
        \end{gather}
      \end{proposition}
    \end{frame}

  \section{The dual inference problem}

    \begin{frame}{BED as probabilistic inference}
      \begin{itemize}[<+->]
        \item Let us introduce a potential function
          \begin{equation}\label{eq:potential-function}
            g_{t}(z_{0:t}) \coloneq \exp \Big\{ \eta \, r_{t}(z_{0:t}) \Big\},
          \end{equation}
          with $\eta \in \mathbb{R}_{>0}$.
        \item We then define a non-Markovian state-space model characterized by the following joint density
          \begin{equation}\label{eq:pathwise_smoothing_trajectory}
            \Gamma_\phi(z_{0:T}) = \frac{1}{Z(\phi)} \, p(z_0) \prod_{t=1}^T p_\phi(z_t \mid z_{0:t-1}) \, g_t(z_{0:t}),
          \end{equation}
        \item The normalization constant is
          \begin{equation}
            Z(\phi) = \int g_{1:T}(z_{0:T}) \, p_\phi(z_{0:T}) \, \dd z_{0:T}.
          \end{equation}
      \end{itemize}
    \end{frame}

    \begin{frame}{BED as probabilistic inference}
      Some observations:\vspace{0.1cm}
      \begin{itemize}
        \setlength\itemsep{1em}
        \item $Z(\phi)$ is analogous to the marginal likelihood $\ell(\phi) = p_\phi(y_{0:T} = 1)$.
        \item $\Gamma_\phi(z_{0:T})$ is analogous to the smoothing distribution $p_\phi(z_{0:T} \mid y_{0:T} = 1)$.
        \item Maximizing the (log) marginal likelihood targets the risk-sensitive objective:
          \begin{align}
            \argmax_{\phi \in \Phi} \,\, \log Z(\phi) &= \argmax_{\phi \in \Phi} \,\, \frac{1}{\eta} \log \mathbb{E}_{p_\phi(z_{0:T})} \left[\exp \left\{\eta \sum_{t=1}^T r_t(z_{0:t})\right\}\right] \\
            &= \mathbb{E} \left[\sum_{t=1}^T r_t(z_{0:t})\right] - \frac{\eta}{2} \mathbb{V} \left[ \sum_{t=1}^T r_t(z_{0:t}) \right] + \mathcal{O}(\eta^2).
          \end{align}
      \end{itemize}
    \end{frame}

    \begin{frame}{BED as probabilistic inference}
      \textcolor{red}{\large What do we know so far?}\vspace{0.2cm}
      \begin{enumerate}
        \setlength\itemsep{1.5em}
        \item<2-> Sequential BED as a maximum likelihood estimation problem.
        \item<3-> MLE with the Markovian score climbing algorithm.
        \item<4-> CSMC kernel from a particle smoother.
      \end{enumerate}

      \vspace{0.3cm}
      \onslide<5->{\textcolor{red}{The one thing we don't know~(yet)} - a particle smoothing algorithm for $\Gamma_\phi(z_{0:T})$.}
    \end{frame}

  \section{Inside-Out \texorpdfstring{SMC\textsuperscript{2}}{SMC2}}
    \begin{frame}{Setting the stage}
      Two components necessary for a bootstrap particle filter:
      \begin{itemize}[<+->]
        \item Sample from the dynamics
          \begin{align}
            z_t &\sim p_\phi(z_t \mid z_{0:t-1}) \\
            &= \pi_\phi(u_{t-1} \mid z_{0:t-1}) \int f(x_t \mid x_{t-1}, u_{t-1}, \theta) \, p(\theta \mid z_{0:t-1}) \, \dd \theta.
          \end{align}
        \item Compute the reward function
          \begin{equation}
            r_t(z_{0:t}) = - \log \int f(x_t \mid x_{t-1}, u_{t-1}, \theta) \, p(\theta \mid z_{0:t-1}) \, \dd \theta.
          \end{equation}
      \end{itemize}
    \end{frame}

    \begin{frame}{Iterated Batch Importance Sampling}
      \begin{itemize}[<+->]
        \setlength\itemsep{1.5em}
        \item For a model
          \begin{gather}
            \theta \sim p(\theta), \\
            z_t \sim p_\phi(z_t \mid z_{0:t-1}, \theta), \quad t \geq 1,
          \end{gather}
          %
          how can we estimate the running filtered posteriors $p(\theta \mid z_{0:t})$?
        \item No surprise - using a particle filter!
        \item We use the \emph{iterated batch importance sampling}~(IBIS) algorithm of~\citet{chopin2002sequential}.
      \end{itemize}
    \end{frame}

    \begin{frame}{Iterated Batch Importance Sampling}
      \begin{columns}
        \begin{column}{0.5\textwidth}
          \begin{itemize}
            \setlength\itemsep{1.5em}
            \only<1-2>{
            \item We start by sampling $M$ many independent and identically distributed samples $\theta^m \sim p(\theta)$ with uniform weights $W^m = 1/M$.
            }
            \only<2>{
            \item We then follow the routine outlined in the function \textsc{IbisStep}.
            }
            \only<3>{
            \item The weight update is in accordance with Bayes' rule
              \begin{equation}
                p(\theta \mid z_{0:t}) \propto p_\phi(z_t \mid z_{0:t-1}, \theta) \, p(\theta \mid z_{0:t-1})
              \end{equation}
            }
          \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
          \only<2->{
          \begin{algorithm}[H]
            \caption{Single step of IBIS.}
            \setstretch{1.35}
            \SetAlgoLined
            \LinesNumbered
            \DontPrintSemicolon
            \SetKwProg{Fn}{Function}{}{}
            \Fn{\textsc{IbisStep}$(z_{0:t}, \theta^{1:M}, W^{1:M})$}{
              Compute $v_t(\theta^m) = p_\phi(z_t \mid z_{0:t-1}, \theta^m)$. \;
              Reweight: $W^m \propto W^m v_t(\theta^m)$. \;
              \If{some degeneracy criterion is fulfilled}{
                Rejuvenate particles using the resample-move algorithm.\;
              }
              \KwRet{$\{\theta^m, W^m\}_{m=1}^M$.} \;
            }
          \end{algorithm}
        }
        \end{column}
      \end{columns}
    \end{frame}

    \begin{frame}{Inside-Out \texorpdfstring{SMC\textsuperscript{2}}{SMC2}}
      \begin{itemize}[<+->]
        \setlength\itemsep{1.5em}
        \item Assume that at time $t$ we have a trajectory $z_{0:t}^n$, where $n = 1, \dots, N$.
        \item Assume that we also have a particle approximation of the filtering posterior of $\theta$ obtained using IBIS:
          \begin{equation}
            p(\theta \mid z_{0:t}^n) \approx \hat{p}(\theta \mid z_{0:t}^n) \coloneq \sum_{m=1}^M W_{t,\theta}^{mn} \, \delta_{\theta_t^{mn}}(\theta).
          \end{equation}
          where $\delta$ is the Dirac delta function.
        \item We can then form an approximation to the marginal dynamics for the state as
          \begin{align}
            \hat{p}(x_{t+1} \mid z_{0:t}^n, u_t) &= \int f(x_{t+1} \mid x_t^n, u_t, \theta) \, \hat{p}(\theta \mid z_{0:t}^n) \, \dd\theta \\
            &= \sum_{m=1}^M W_{t,\theta}^{mn} \, f(x_{t+1} \mid x_t^n, u_t, \theta_t^{mn}).
      \end{align}
      \end{itemize}
    \end{frame}
    
    \begin{frame}{Inside-Out \texorpdfstring{SMC\textsuperscript{2}}{SMC2}}
      \begin{center}
        \begin{minipage}{0.9\textwidth}
          \begin{algorithm}[H]
            \caption{Inside-Out SMC\textsuperscript{2}}
            \setstretch{1.35}
            \LinesNumbered
            \SetAlgoLined
            \DontPrintSemicolon
            Sample $z_0^n \sim p(z_0)$, $\theta_0^{mn} \sim p(\theta)$ and set $W_{0, \theta}^{mn} \gets 1/M$. \;
            Sample $z_1^n \sim \hat{p}_\phi(\cdot \mid z_0^n)$ and initialize the state history $z_{0:1}^n \gets (z_0^n, z_1^n)$. \;
            Compute and normalize the weights
                \vspace{-0.2cm} $$\smash{W_z^n \propto \exp \Big \{-\eta \log \hat{p}(x_1^n \mid x_0^n, u_0^n) \Big\}.}$$ \vspace{-0.6cm}\;
            \For{$t \gets 1, \dots, T - 1$}{
              Sample $b_t^n \sim \mathcal{M}(W_z^{1:N})$. \;
              $\theta_t^{\bullet n}, W^{\bullet n}_{t, \theta}, \gets \textsc{Ibis\_Step}(z_{0:t}^{b_t^n}, \theta_{t-1}^{\bullet b_t^n}, W^{\bullet b_t^n}_{t - 1, \theta})$ \;
              Sample $z_{t+1}^n \sim \hat{p}_\phi(\cdot \mid z_{0:t}^{b_t^n}),$
                  and append to state history $z_{0:t+1}^n \gets [z_{0:t}^{b_t^n}, z_{t+1}^n]$. \;
              Compute and normalize the weights
                  \vspace{-0.2cm} $$\smash{W_z^n \propto \exp\left\{-\eta \log \hat{p}(x_{t+1}^n \mid z_{0:t}^{b_t^n}, u_t^n)\right\}.}$$ \vspace{-0.6cm} \;
            }
            \KwRet{$\{z_{0:T}^n, W_z^n\}_{n=1}^N$.}
            \label{alg:iosmc}
          \end{algorithm}
        \end{minipage}
      \end{center}
    \end{frame}

    \begin{frame}{Inside-Out \texorpdfstring{SMC\textsuperscript{2}}{SMC2}}
      \begin{itemize}
        \setlength\itemsep{1.5em}
        \item The algorithm asymptotically targets the distribution $\Gamma_\phi(z_{0:T})$~(Proposition 2).
        \item A conditional version of the algorithm can be developed in the same way~(Appendix C.4).
        \item This conditional IO-SMC\textsuperscript{2} algorithm can be used in place of standard CSMC in the MSC algorithm from the previous part.
        \item This will target the maximum likelihood estimate for $Z(\phi)$.
      \end{itemize}
    \end{frame}

    \begin{frame}{Preliminary Empirical Results}
      \begin{figure}[t]
        \input{figures/pendulum_info_gain}
        \vspace{-0.25cm}
        \caption{Information gain of different policies computed in closed-form on the conditionally-linear stochastic pendulum. We report the mean and standard deviation over 512 realizations.}
        \label{fig:pendulum_info_gain}
      \end{figure}
    \end{frame}

    \begin{frame}{Preliminary Empirical Results}
      \begin{figure}[!h]
        \centering
        \input{figures/pendulum_sample_trajectory}
        \vspace{-0.25cm}
        \caption{A sample experiment trajectory generated by the amortized policy during deployment on the non-linear stochastic pendulum environment. $q$ is the angle of the pendulum from the vertical, $\dot{q}$ is the angular velocity and $u$ is the action.}
      \end{figure}
    \end{frame}

    \begin{frame}{Preliminary Empirical Results}
      \begin{figure}[t]
        \input{figures/pendulum_eig}
        \caption{Training progression of the IO-SMC\textsuperscript{2} policy and its exact variant on the conditionally-linear stochastic pendulum. At every epoch, we evaluate the EIG estimate using the mean policy. We report the mean and standard deviation over 25 training runs.}
      \end{figure}
    \end{frame}

    \begin{frame}{Summary}
      \textbf{Strengths}:\vspace{1em}
      \begin{itemize}
        \setlength\itemsep{1em}
        \item A novel approach to sequential Bayesian experimental design.
        \item A general particle filter for non-Markovian time series models.
        \item Most existing work use a high-bias estimator of the EIG.
      \end{itemize}
      \vspace{1em}
      \textbf{Limitations}:\vspace{1em}
      \begin{itemize}
        \setlength\itemsep{1em}
        \item Empirical validation is insufficient (currently ongoing).
        \item Principled way to choose the temperature.
        \item Full access to the transition density.
      \end{itemize}
    \end{frame}

  % Bibliography
  \begin{frame}[t, allowframebreaks]{References}
    \bibliography{references}
  \end{frame}

  \appendix

  \begin{frame}{IBIS}
    \begin{columns}
      \begin{column}{0.5\textwidth}
        \begin{itemize}
          \only<1-2>{
          \item We start by sampling $M$ many independent and identically distributed samples $\theta^m \sim p(\theta)$ with uniform weights $W^m = 1/M$.
          }
          \only<2>{
          \item We then follow the routine outlined in the function \textsc{IbisStep}.
          }
          \only<3>{
          \item The weight update is in accordance with Bayes' rule
            \begin{equation}
              p(\theta \mid z_{0:t}) \propto p_\phi(z_t \mid z_{0:t-1}, \theta) \, p(\theta \mid z_{0:t-1})
            \end{equation}
          }
          \only<4>{
          \item The degeneracy criterion we use is
            \begin{equation}
              \textrm{ESS} < 0.75 * M,
            \end{equation}
            where ESS stands for effective sample size, which is calculated as
            \begin{equation}
              \textrm{ESS} = \frac{1}{\sum_{m=1}^M (W^m)^2}
            \end{equation}
          }
          \only<5>{
          \item $K_t$ is a $p(\theta \mid z_{0:t})$-ergodic MCMC kernel.
          \item We use a random walk Metropolis Hastings kernel.
          }
        \end{itemize}
      \end{column}
      \begin{column}{0.5\textwidth}
        \only<2->{
        \begin{algorithm}[H]
          \caption{Single step of IBIS.}
          \setstretch{1.35}
          \SetAlgoLined
          \LinesNumbered
          \DontPrintSemicolon
          \SetKwProg{Fn}{Function}{}{}
          \Fn{\textsc{IbisStep}$(z_{0:t}, \theta^{1:M}, W^{1:M})$}{
            Compute $v_t(\theta^m) = p_\phi(z_t \mid z_{0:t-1}, \theta^m)$. \;
            Reweight: $W^m \propto W^m v_t(\theta^m)$. \;
            \If{some degeneracy criterion is fulfilled}{
              Resample: $a_t^m \sim \mathcal{M}(W^{1:M})$. \;
              Move: $\tilde{\theta}^m \sim K_t(\theta^{a_t^m}, \cdot)$. \;
              Replace the current set of weighted particles with
              $(\theta^m, W^m) \gets (\tilde{\theta}^m, 1 / M)$. \;
            }
            \KwRet{$\{\theta^m, W^m\}_{m=1}^M$.} \;
          }
        \end{algorithm}
      }
      \end{column}
    \end{columns}
  \end{frame}

  \begin{frame}{Preliminaries - Genealogy Tracking}
    \begin{columns}
      \begin{column}{0.5\textwidth}
        \begin{itemize}
          \item<1-> How do we obtain samples distributed according to $p_\phi(z_{0:T} \given y_{0:T})$?
          \item<2-> The particle filter generates smoothed trajectories "for free".
          \vspace{1cm}
        \end{itemize}          
      \end{column}
      \begin{column}<2->{0.5\textwidth}
        \begin{figure}[htbp]
          \centering
          \includegraphics[width=0.7\textwidth]{figures/genealogy_tracking.png}
          \caption{Ancestral lineages generated by an SMC algorithm~(from \citet{andrieu2010particle}).}
        \end{figure}
        % \begin{algorithm*}[H]
        %   \setstretch{1.35}
        %   \SetAlgoLined
        %   \LinesNumbered
        %   \DontPrintSemicolon
        %   Sample $B_T$ with $\mathbb{P}(B_T = k) \propto w_T^k$ \; 
        %   Set $z_T^* = z_T^{B_T}$ \;
        %   \For{$t \gets T-1, \dots, 0$}{
        %     Set $B_t = A_{t+1}^{B_{t+1}}$. \;
        %     Set $z_t^* = z_t^{B_t}$. \; 
        %   }
        %   \caption{Genealogy tracking.}
        % \end{algorithm*}
      \end{column}
    \end{columns}
  \end{frame}

  \begin{frame}{Preliminaries - Backward sampling}
    \begin{columns}
      \begin{column}{0.5\textwidth}
        \begin{itemize}
          \item<1-> If we can compute the transition density in closed form, we can do better using the \emph{backward sampling} routine.
          \item<2-> The result is the \emph{forward filtering backward smoothing} algorithm.
        \end{itemize}          
      \end{column}
      \begin{column}<1->{0.5\textwidth}
        \begin{algorithm*}[H]
          \setstretch{1.35}
          \SetAlgoLined
          \LinesNumbered
          \DontPrintSemicolon
          Sample $B_T$ with $\mathbb{P}(B_T = k) \propto w_T^k$ \; 
          Set $z_T^* = z_T^{B_T}$ \;
          \For{$t \gets T-1, \dots, 0$}{
            Sample $B_t$ with $\mathbb{P}(B_t = k) \propto w_t^k \, f_\phi(z_{t+1}^{*} \given z_t^k)$ \; 
            Set $z_t^* = z_t^{B_t}$ \; 
          }
        \end{algorithm*}
      \end{column}
    \end{columns}
  \end{frame}


\end{document}
