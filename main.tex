\documentclass[10pt, aspectratio=1610]{beamer}

% Theming
\usetheme{Madrid}
\usecolortheme{beaver}
\usefonttheme{serif}

\usepackage{amsmath,amssymb,amsthm,mathtools}
\usepackage{algorithm2e, setspace}
\mathtoolsset{showonlyrefs}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\given}{\,|\,}
\newcommand{\dd}{\mathrm{d}}
\newtheorem{proposition}[theorem]{Proposition}

\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}

% Natbib for citations.
\usepackage{natbib}
\bibliographystyle{unsrtnat}

\title{Policy Optimization with Markovian Score Climbing}
\date{February 16, 2024}
\author{Sahel Iqbal}
\institute{Aalto University, Finland}

\begin{document}
  \maketitle

  \AtBeginSection[] {
    \begin{frame}{Table of Contents}
		\tableofcontents[currentsection]
	\end{frame}
  }
  
  \begin{frame}{Works covered in this talk}
    \begin{enumerate}
      \item \emph{Risk-Sensitive Stochastic Optimal Control as Rao-Blackwellized Markovian Score Climbing}. Hany Abdulsamad, Sahel Iqbal, Adrien Corenflos, Simo S\"arkk\"a. 2023. arXiv preprint arXiv:2312.14000.
      \item \emph{Nesting Particle Filters for Experimental Design in Dynamical Systems}. Sahel Iqbal, Adrien Corenflos, Simo S\"arkk\"a, Hany Abdulsamad. 2024. arXiv preprint arXiv:2402.07868.
    \end{enumerate}
  \end{frame}

  \section{Stochastic Optimal Control as Inference}
    \begin{frame}{Decision Making under Uncertainty}
      Consider the discrete-time sequential decision making problem
      \begin{equation}\label{eq:soc_objective}
        \mathcal{J}(\pi) = \mathbb{E}_{x_{0:T}, u_{0:T}} \left[\sum_{t=0}^{T} c(x_t, u_t)\right],
      \end{equation}
      %
      where
      \begin{itemize}
        \item $x_t \in \mathbb{R}^m$ denote the states.
        \item $u_t \in \mathbb{R}^n$ denote the actions.
        \item $c(x_t, u_t) \geq 0$ is the stage cost.
      \end{itemize}
  
      \vskip 1cm
      The joint distribution of states and actions is
      \begin{equation}
          p(x_{0:T}, u_{0:T}) = \mu(x_{0}) \prod_{t=0}^{T-1} f(x_{t+1} \mid x_t, u_t) \, \prod_{t=0}^T \pi(u_t \mid x_t).
      \end{equation}
    \end{frame}

    \begin{frame}{Decision Making under Uncertainty}
        For a policy space $\Pi$, we want to find
        \begin{equation*}
            \pi^{*} = \argmin_{\pi \in \Pi} \medspace \mathcal{J}(\pi).
        \end{equation*}
        ~ \\
        For a parametric form $\pi(\cdot \given x_t) \equiv \pi_{\phi}(\cdot \given x_t)$ with $\phi \in \Phi \subseteq \mathbb{R}^{l}$
        \begin{equation*}
            \phi^{*} = \argmin_{\phi \in \Phi} \medspace \mathcal{J}(\phi).
        \end{equation*}
    \end{frame}

    \begin{frame}{The pseudo-likelihood}
      \begin{columns}
        \begin{column}{0.6\textwidth}
          \begin{itemize}
            \item Following \citet{toussaint2006probabilistic} and \citet{rawlik2013probabilistic}, let us introduce a binary random variable $y_t$ with likelihood
              \begin{equation}
                \smash{g(y_t = 1 \mid x_t, u_t) = \exp \big\{-\eta \, c(x_t, u_t)\big\}},
              \end{equation}
              %
              where $\eta \in \mathbb{R}_{>0}$.
            \item This can be interpreted as the \emph{probability of being optimal}.
            \item We can treat this as a pseudo-observation in a state-space model.
          \end{itemize}
        \end{column}
        \begin{column}{0.4\textwidth}
          \begin{figure}[htbp]
            \label{fig:dgm}
            \centering
            \include{figures/dgm}
            \caption{The directed graphical model underlying the control problem.}
          \end{figure}
        \end{column}
      \end{columns}
    \end{frame}

    \begin{frame}{Maximum Likelihood}
      Maximizing the marginal likelihood is equivalent to minimizing a risk-sensitive objective
      \begin{align}\label{eq:max_likelihood}
        \argmax_{\phi \in \Phi} \medspace \log p(y_{0:T} = 1 \mid \phi) &= \argmin_{\phi \in \Phi} \medspace -\frac{1}{\eta}\log \mathbb{E}_{p_{\phi}} \left[ \exp \big\{-\eta \textstyle \sum_{t=0}^{T} c(x_{t}, u_{t}) \big\} \right] \\
        &\coloneq \argmin_{\phi \in \Phi} \mathcal{J}_\eta(\phi)
      \end{align}
      %
      where $p_{\phi} \coloneqq p(x_{0:T}, u_{0:T} \mid \phi)$.

      \vskip 0.5cm
      Taylor expansion around $\eta = 0$ yields
      \begin{equation}
        \mathcal{J}_\eta(\phi) = \mathbb{E} \Bigg[ \sum_{t=0}^{T} c_{t}(x_{t}, u_{t}) \Bigg] - \frac{\eta}{2} \, \mathbb{V} \Bigg[ \sum_{t=0}^{T} c_{t}(x_{t}, u_{t}) \Bigg] + \mathcal{O}(\eta^{2}).
      \end{equation}
      %
      Thus, this log-marginal objective amounts to a risk-sensitive variant of the control objective modulated by the temperature $\eta$.
    \end{frame}

    \begin{frame}{Cleaning up the notation}
      \begin{itemize}
        \item Let us define an augmented state
          \begin{equation}
            z_t \coloneq (x_t, u_t).
          \end{equation}
        \item Additionally, let us define
          \begin{equation}
            \mu_{\phi}(z_0) \coloneqq \mu(x_0) \, \pi_{\phi}(u_0 \mid x_0), \quad f_{\phi}(z_{t+1} \mid z_t) \coloneqq f(x_{t+1} \mid x_t, u_t) \, \pi_{\phi}(u_{t+1} \mid x_{t+1}).
          \end{equation}
        \item We can then write a joint density
          \begin{equation}\label{eq:paper_1_ssm}
            p_{\phi}(z_{0:T}, y_{0:T}) = \mu_{\phi}(z_0) \prod_{t=0}^{T-1} f_{\phi}(z_{t+1} \mid z_t) \prod_{t=0}^T g(y_t \mid z_t).
          \end{equation}
        \item Our goal is to find the maximum likelihood estimate for the SSM specified by~\eqref{eq:paper_1_ssm}.
      \end{itemize}
    \end{frame}

    \begin{frame}{Complications}
      \begin{equation}
        p_{\phi}(z_{0:T}, y_{0:T}) = \mu_{\phi}(z_0) \prod_{t=0}^{T-1} f_{\phi}(z_{t+1} \mid z_t) \prod_{t=0}^T g(y_t \mid z_t).
      \end{equation}
      \begin{itemize}
        \item In general, the SSM can be
        \begin{itemize}
          \item non-linear,
          \item non-Gaussian,
          \item bounded support.
        \end{itemize}
      \item Gaussian approximations yield biased estimates of the marginal likelihood.
      \item The solution - particle methods!
      \end{itemize}
    \end{frame}

  \section{Markovian Score Climbing for MLE}
    \begin{frame}{Preliminaries - Particle filters}
      \begin{columns}
        \begin{column}{0.5\textwidth}
          \begin{itemize}
            \item A \emph{particle filter} is a sequential Monte Carlo algorithm that can be used to estimate
              \begin{itemize}
                \item the pathwise filtering distributions $p_\phi(x_{0:t} \mid y_{0:t})$,
                \item and the marginal likelihoods $p_\phi(y_{0:t})$.
              \end{itemize}
            \item A more descriptive name for the particle filter - \emph{sequential importance resampling}.
          \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
          \begin{algorithm}[H]
            \setstretch{1.35}
            \SetAlgoLined
            \LinesNumbered
            \DontPrintSemicolon
            \For{$n \gets 1, \dots, N$}{
              Sample $z_0^n \sim \mu_\phi(\cdot)$ and $w_0^n = g(y_0 \given z_0^n)$. \;
              }
              \For{$t \gets 1, \dots, T$}{
                \For{$n \gets 1, \dots, N$}{
                  \tcp{Resample}
                  Sample $A_t^n$ with $\mathbb{P}(A_t^n = k) \propto w_{t-1}^k$. \;
                  \tcp{Propagate}
                  Sample $z_t^n \sim f_\phi(\cdot \given z_{t-1}^{A_t^n})$. \;
                  \tcp{Reweight}
                  Set $w_t^n = g(y_t \given z_t^n)$.\;
                }
              }
              \label{alg:particle_filter}
              \caption{Bootstrap particle filter.}
          \end{algorithm}
        \end{column}
      \end{columns}
    \end{frame}

    \begin{frame}{Preliminaries - Particle filters}
      \begin{itemize}
        \item Particle filters can provide unbiased estimates of the marginal likelihood for general SSMs, but obtaining its gradient is usually not possible.
        \item In literature this difficulty is often circumvented using Fisher's identity:
          \begin{equation}
            \nabla_\phi \, \ell(\phi) \coloneq \nabla_\phi \, p_\phi(y_{0:T} = 1) = \int \nabla_\phi \log p_{\phi}(z_{0:T}, y_{0:T}) \, p_\phi(z_{0:T} \given y_{0:T}) \, \dd z_{0:T}.
          \end{equation}
        \item The above integral can be estimated using samples from the smoothing distribution.
      \end{itemize} 
    \end{frame}

    \begin{frame}{Preliminaries - Genealogy tracking}
      \begin{columns}
        \begin{column}{0.5\textwidth}
          \begin{itemize}
            \item<1-> How do we obtain samples distributed according to $p_\phi(z_{0:T} \given y_{0:T})$?
            \item<2-> The particle filter generates smoothed trajectories "for free".
            \vspace{1cm}
          \end{itemize}          
        \end{column}
        \begin{column}<2->{0.5\textwidth}
          \begin{figure}[htbp]
            \centering
            \includegraphics[width=0.7\textwidth]{figures/genealogy_tracking.png}
            \caption{Ancestral lineages generated by an SMC algorithm~(from \citet{andrieu2010particle}).}
          \end{figure}
          % \begin{algorithm*}[H]
          %   \setstretch{1.35}
          %   \SetAlgoLined
          %   \LinesNumbered
          %   \DontPrintSemicolon
          %   Sample $B_T$ with $\mathbb{P}(B_T = k) \propto w_T^k$ \; 
          %   Set $z_T^* = z_T^{B_T}$ \;
          %   \For{$t \gets T-1, \dots, 0$}{
          %     Set $B_t = A_{t+1}^{B_{t+1}}$. \;
          %     Set $z_t^* = z_t^{B_t}$. \; 
          %   }
          %   \caption{Genealogy tracking.}
          % \end{algorithm*}
        \end{column}
      \end{columns}
    \end{frame}

    \begin{frame}{Preliminaries - Backward sampling}
      \begin{columns}
        \begin{column}{0.5\textwidth}
          \begin{itemize}
            \item If we can compute the transition density in closed form, we can do better.
            \item The result is the \emph{forward filtering backward smoothing} algorithm.
          \end{itemize}          
        \end{column}
        \begin{column}{0.5\textwidth}
          \begin{algorithm*}[H]
            \setstretch{1.35}
            \SetAlgoLined
            \LinesNumbered
            \DontPrintSemicolon
            Sample $B_T$ with $\mathbb{P}(B_T = k) \propto w_T^k$ \; 
            Set $z_T^* = z_T^{B_T}$ \;
            \For{$t \gets T-1, \dots, 0$}{
              Sample $B_t$ with $\mathbb{P}(B_t = k) \propto w_t^k \, f_\phi(z_{t+1}^{*} \given z_t^k)$ \; 
              Set $z_t^* = z_t^{B_t}$ \; 
            }
            \caption{Backward sampling.}
          \end{algorithm*}
        \end{column}
      \end{columns}
    \end{frame}

    \begin{frame}{Preliminaries - Gradient ascent using particle smoothing}
      We now have the building blocks in place to perform MLE through a gradient ascent procedure:
      \begin{equation}
          \phi_k = \phi_{k-1} + \gamma_k \, \widehat{\nabla_\phi \ell}(\phi_{k-1}),
      \end{equation}
      where
      \begin{itemize}
        \item $\widehat{\nabla_\phi \ell}(\phi_{k-1})$ is our stochastic estimate of the score function.
        \item $\gamma_k$ are step sizes that satisfy $\sum_{k=1}^\infty \gamma_k = \infty, \quad \sum_{k=1}^\infty \gamma_k^2 < \infty$.
      \end{itemize}

      \vspace{0.5cm}
      However, one more necessary criterion for the convergence of stochastic gradient ascent~\citep{robbins1951stochastic}, namely
      \begin{equation}
        \mathbb{E}\left[\widehat{\nabla_\phi \ell}(\phi_{k-1})\right] = \nabla_\phi \ell(\phi_{k-1})
      \end{equation}
    \end{frame}

\begin{frame}{Markovian Score Climbing For Likelihood Maximization}
        \begin{itemize}
            \setlength\itemsep{2em}
            \item Estimates of $\nabla_\phi \, \ell(\phi)$ via particle smoothing are biased for a finite number of particles $N$.
            \item We can do better by drawing samples from a Markov chain Monte Carlo kernel $\mathcal{K}$ ergodic w.r.t. $p_\phi(z_{0:T} \given y_{0:T})$, i.e.,
            \begin{equation*}
                Z^n \sim \mathcal{K}(\cdot \given \mathcal{K}(\cdot \given \ldots, \phi), \phi) := \mathcal{K}^n(\cdot \given z_{0:T}, \phi).
            \end{equation*}
            \item The $p_\phi(z_{0:T} \given y_{0:T})$-ergodic MCMC kernel $\mathcal{K}$ guarantees consistent estimates, no matter the number of samples $N$.
        \end{itemize}
    \end{frame}
\begin{frame}{Markovian Score Climbing For Likelihood Maximization}
        \begin{algorithm}[H]
            \setstretch{1.35}
            \SetAlgoLined
            \LinesNumbered
            \DontPrintSemicolon
            \KwIn{Initial trajectory $z^{0}_{0:T}$, initial parameters $\phi_0$, number of iterations $M$, step sizes $\gamma_{1:M}$, Markov kernel $\mathcal{K}$.}
            \KwOut{$\phi^* \approx \phi_{\text{MLE}}$}
            \For{$k \gets 1,\dots, M$}{
                Sample $z^{k}_{0:T} \sim \mathcal{K}(\cdot \given z^{k-1}_{0:T}, \phi_{k-1})$ \;
                Compute $\widehat{\nabla_\phi \ell}(\phi_{k-1}) \gets \nabla_\phi \log p_\phi(z^{k}_{0:T}, y_{0:T}) \vert_{\phi=\phi_{k-1}}$ \;
                Update $\phi_k \gets \phi_{k-1} + \gamma_k \widehat{\nabla_\phi \ell}(\phi_{k-1})$\;
            }
            \Return{$\phi_M$}
        \end{algorithm}
    \end{frame}

    \begin{frame}{\textcolor{blue}{Conditional} SMC with backward sampling}
        \begin{algorithm}[H]
            \setstretch{1.35}
            \SetAlgoLined
            \LinesNumbered
            \DontPrintSemicolon
            \KwIn{\textcolor{blue}{Reference trajectory $z_{0:T}$}}
            \tcp{Forward filtering}
            \textcolor{blue}{Set $z_0^1 = z_0$ and $w_0^1 = g(y_0 \given z_0)$} \;
            \For{$n \gets 2, \dots, N$}{
            Sample $z_0^n \sim \mu_\phi(\cdot)$ and $w_0^n = g(y_0 \given z_0^n)$ \;
            }
            \For{$t \gets 1, \dots, T$}{
                \textcolor{blue}{Set $z_t^1 = z_t$ and $w_t^1 = g(y_t \given z_t)$} \;
                \For{$n \gets 2, \dots, N$}{
                    Sample $A_t^n$ with $\mathbb{P}(A_t^n = k) \propto w_{t-1}^k$ \;
                    Sample $z_t^n \sim f_\phi(\cdot \given z_{t-1}^{A_t^n})$ and $w_t^n = g(y_t \given z_t^n)$\;
                }
            }
        \end{algorithm}
    \end{frame}

    \begin{frame}{Rao-Blackwellized Markovian Score Climbing}
        \begin{algorithm}[H]
            \setstretch{1.5}
            \SetAlgoLined
            \LinesNumbered
            \DontPrintSemicolon
            \KwIn{Trajectory $z_{0:T}$}
            \KwOut{Score estimate $\widehat{\nabla_\phi \ell}(\phi)$}
            Run forward filtering to obtain the filtered particles, their weights, and the resampling indices \;
            Backward-Sample $z^{n}_{0:T} \sim \mathcal{B}$ for all $n = 1, \dots, N$ \;
            Compute $\widehat{\nabla_\phi \ell}(\phi) \gets \frac{1}{N} \sum_{n=1}^N \nabla_\phi \log p_\phi(z^{n}_{0:T}, y_{0:T})$ \;
            \Return{$\widehat{\nabla_\phi \ell}(\phi)$}
        \end{algorithm}
    \end{frame}

    \begin{frame}{Numerical Validation}
      \begin{columns}
        \begin{column}{0.5\textwidth}
          \begin{figure}[htbp]
            \centering
            \include{figures/pendulum}
          \end{figure}
        \end{column}
        \begin{column}{0.5\textwidth}
          \begin{figure}[htbp]
            \centering
            \include{figures/cartpole}
          \end{figure}
        \end{column}
      \end{columns}
    \end{frame}

    \begin{frame}{Numerical Validation}
      \begin{columns}
        \begin{column}{0.5\textwidth}
          \begin{figure}[htbp]
            \centering
            \include{figures/double_pendulum}
          \end{figure}
        \end{column}
        \begin{column}{0.5\textwidth}
          \begin{figure}[htbp]
            \centering
            \includegraphics[scale=0.1]{figures/psoc_qr.png}
            \caption{\url{github.com/hanyas/psoc.git}}
          \end{figure}
        \end{column}
      \end{columns}
    \end{frame}

    \section{Bayesian Experimental Design}
    \begin{frame}{Bayesian Experimental Design}
      Let
      \begin{itemize}
        \item $\theta \in \Theta$ be a set of unknown parameters of interest with prior $p(\theta)$.
        \item $\xi \in \Xi$ be a user controllable \emph{design}.
        \item $p(x \mid \xi, \theta)$ be a likelihood function.
      \end{itemize}
      
      \pause
      \begin{definition}
        The \emph{information gain}~\citep[IG,][]{lindley1956measure}, in the parameter $\theta$, on applying the design $\xi$ and observing the outcome $x$ is defined as
        \begin{equation}\label{eq:ig_single_experiment}
          \mathcal{G}(x, \xi) \coloneq \mathbb{H}[p(\theta)] - \mathbb{H}[p(\theta\mid x, \xi)].
        \end{equation}
      \end{definition}

      \pause
      \begin{definition}
        The \emph{expected information gain}~(EIG) is defined as
        \begin{equation}
          \mathcal{I}(\xi) \coloneq \mathbb{E}_{p(x \mid \xi)} \, \mathcal{G}(x, \xi).
        \end{equation}
      \end{definition}
    \end{frame}

    \begin{frame}{Sequential Bayesian Experimental Design}
      Let us define $z_0 \coloneq \{x_0\}$ and $z_{t} \coloneq \{x_t, \xi_{t-1}\}$ for all $t \geq 1$, and denote the outcome-design history up to time $t$ by $z_{0:t} \coloneq \{x_{0:t}, \xi_{0:t-1}\}$. The system dynamics is specified by
      \begin{gather}
        x_0 \sim p(x_0), \\
        \xi_{t-1} \sim \pi_\phi(\xi_{t-1} \mid z_{0:t-1}), \\
        x_t \sim f(x_t \mid x_{t-1}, \xi_{t-1}, \theta),
      \end{gather}
      \pause

      \vspace{0.2cm}
      The joint density of states and designs is hence
      \begin{align}\label{eq:joint_density}
        p_{\phi}(z_{0:T} \mid \theta) &= p(z_{0}) \prod_{t=1}^T p_{\phi}(z_{t} \mid z_{0:t-1}, \theta) \\
        &\coloneq p(x_0) \prod_{t=1}^T f(x_t \mid x_{t-1}, \xi_{t-1}, \theta) \, \pi_\phi(\xi_{t-1} \mid z_{0:t-1})
      \end{align}
    \end{frame}

    \begin{frame}{Sequential Bayesian Experimental Design}
      \begin{itemize}
        \item The EIG for the sequential problem is
          \begin{equation}
            \mathcal{I}(\phi) \coloneq \mathbb{E}_{p_{\phi}(z_{0:T})} \Bigl[ \mathbb{H}\bigl[p(\theta)\bigr] - \mathbb{H}\bigl[p(\theta \mid z_{0:T})\bigr] \Bigr].
          \end{equation}
        \item Our goal is to find optimal policy parameters $\phi^*$ such that
          \begin{equation}
            \phi^* \coloneq \argmax_{\phi \in \Phi} \mathcal{I}(\phi)
          \end{equation}
      \end{itemize}
    \end{frame}

    \begin{frame}{Sequential Bayesian Experimental Design}
      \begin{proposition}
        For models with additive, constant noise in the dynamics, the EIG can be written as
        \begin{equation}\label{eq:eig_constant_noise}
            \mathcal{I}(\phi) \equiv \mathbb{E}_{p_{\phi}(z_{0:T})} \left[ \sum_{t=1}^T r_{t}(z_{0:t}) \right],
        \end{equation}
        where
        \begin{gather}
          r_{t}(z_{0:t}) = - \log \int p(\theta \mid z_{0:t-1}) \, f(x_t \mid x_{t-1}, \xi_{t-1}, \theta) \, \dd \theta, \\
          p_\phi(z_{0:T}) = \int p_\phi(z_{0:T} \mid \theta) \, p(\theta) \, \dd \theta.
        \end{gather}
      \end{proposition}
    \end{frame}

  \section{The dual inference problem}

    \begin{frame}{BED as probabilistic inference}
      Let us introduce a potential function
      \begin{equation}\label{eq:potential-function}
        g_{t}(z_{0:t}) \coloneq \exp \Big\{ \eta \, r_{t}(z_{0:t}) \Big\},
      \end{equation}
      with $\eta \in \mathbb{R}_{>0}$.

      \vspace{0.3cm}
      We then define a non-Markovian state-space model characterized by the following joint density
      \begin{equation}\label{eq:pathwise_smoothing_trajectory}
        \Gamma_\phi(z_{0:T}) = \frac{1}{Z(\phi)} \, p(z_0) \prod_{t=1}^T p_\phi(z_t \mid z_{0:t-1}) \, g_t(z_{0:t}),
      \end{equation}
      where the normalization constant is
      \begin{equation}
        Z(\phi) = \int g_{1:T}(z_{0:T}) \, p_\phi(z_{0:T}) \, \dd z_{0:T}.
      \end{equation}
    \end{frame}

    \begin{frame}{BED as probabilistic inference}
      Some observations:
      \begin{itemize}
        \item $Z(\phi)$ is analogous to the marginal likelihood $\ell(\phi) = p_\phi(y_{0:T} = 1)$.
        \item $\Gamma_\phi(z_{0:T})$ is analogous to the smoothing distribution $p_\phi(z_{0:T} \mid y_{0:T} = 1)$.
        \item Maximizing the (log) marginal likelihood targets the risk-sensitive objective:
          \begin{align}
            \argmax_{\phi \in \Phi} \log Z(\phi) &= \argmax_{\phi \in \Phi} \frac{1}{\eta} \log \mathbb{E}_{p_\phi(z_{0:T})} \left[\exp \left\{\eta \sum_{t=1}^T r_t(z_{0:t})\right\}\right] \\
            &= \mathbb{E} \left[\sum_{t=1}^T r_t(z_{0:t})\right] - \frac{\eta}{2} \mathbb{V} \left[ \sum_{t=1}^T r_t(z_{0:t}) \right] + \mathcal{O}(\eta^2).
          \end{align}
      \end{itemize}
    \end{frame}

    \begin{frame}{BED as probabilistic inference}
      \textcolor{red}{\large What do we know so far?}\vspace{0.1cm}
      \begin{enumerate}
        \item<2-> We know how to formulate the sequential BED problem as a maximum likelihood estimation problem (in a non-Markovian state-space model).
        \item<3-> We know how to perform MLE with the Markovian score climbing algorithm, given an MCMC kernel that targets the pathwise smoothing distribution.
        \item<4-> We know how to convert a particle smoothing algorithm to a CSMC kernel.
      \end{enumerate}

      \vspace{0.3cm}
      \onslide<5->{\textcolor{red}{The one thing we don't know~(yet)} - a particle smoothing algorithm for $\Gamma_\phi(z_{0:T})$.}
    \end{frame}

  \section{Inside-Out \texorpdfstring{SMC\textsuperscript{2}}{SMC2}}
    \begin{frame}{Setting the stage}
      To come up with a bootstrap particle filter for our non-Markovian time series model, we need to be able to sample from the marginal dynamics,
      \begin{align}
        z_t &\sim p_\phi(z_t \mid z_{0:t-1}) \\
        &= \pi_\phi(\xi_{t-1} \mid z_{0:t-1}) \int f(x_t \mid x_{t-1}, \xi_{t-1}, \theta) \, p(\theta \mid z_{0:t-1}) \, \dd \theta,
      \end{align}
      %
      and compute the reward function,
      \begin{equation}
        r_t(z_{0:t}) = - \log \int f(x_t \mid x_{t-1}, \xi_{t-1}, \theta) \, p(\theta \mid z_{0:t-1}) \, \dd \theta.
      \end{equation}
      \pause
      Both of these require estimating the filtered posteriors $p(\theta \mid z_{0:t-1})$.
    \end{frame}

    \begin{frame}{IBIS}
      \begin{itemize}[<+->]
        \item For a model
          \begin{gather}
            \theta \sim p(\theta), \\
            z_t \sim p_\phi(z_t \mid z_{0:t-1}, \theta), \quad t \geq 1,
          \end{gather}
          %
          how can we estimate the running filtered posteriors $p(\theta \mid z_{0:t})$?
        \item No surprise - using a particle filter!
        \item We use the \emph{iterated batch importance sampling}~(IBIS) algorithm of~\citet{chopin2002sequential}.
      \end{itemize}
    \end{frame}

    \begin{frame}{IBIS}
      \begin{columns}
        \begin{column}{0.5\textwidth}
          \begin{itemize}
            \only<1-2>{
            \item We start by sampling $M$ many independent and identically distributed samples $\theta^m \sim p(\theta)$ with uniform weights $W^m = 1/M$.
            }
            \only<2>{
            \item We then follow the routine outlined in the function \textsc{IbisStep}.
            }
            \only<3>{
            \item The weight update is in accordance with Bayes' rule
              \begin{equation}
                p(\theta \mid z_{0:t}) \propto p_\phi(z_t \mid z_{0:t-1}, \theta) \, p(\theta \mid z_{0:t-1})
              \end{equation}
            }
            \only<4>{
            \item The degeneracy criterion we use is
              \begin{equation}
                \textrm{ESS} < 0.75 * M,
              \end{equation}
              where ESS stands for effective sample size, which is calculated as
              \begin{equation}
                \textrm{ESS} = \frac{1}{\sum_{m=1}^M (W^m)^2}
              \end{equation}
            }
            \only<5>{
            \item $K_t$ is a $p(\theta \mid z_{0:t})$-ergodic MCMC kernel.
            \item We use a random walk Metropolis Hastings kernel.
            }
          \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
          \uncover<2->{
          \begin{algorithm}[H]
            \setstretch{1.35}
            \SetAlgoLined
            \LinesNumbered
            \DontPrintSemicolon
            \SetKwProg{Fn}{Function}{}{}
            \Fn{\textsc{IbisStep}$(z_{0:t}, \theta^{1:M}, W^{1:M})$}{
              Compute $v_t(\theta^m) = p_\phi(z_t \mid z_{0:t-1}, \theta^m)$. \;
              Reweight: $W^m \propto W^m v_t(\theta^m)$. \;
              \If{some degeneracy criterion is fulfilled}{
                Resample: $a_t^m \sim \mathcal{M}(W^{1:M})$. \;
                Move: $\tilde{\theta}^m \sim K_t(\theta^{a_t^m}, \cdot)$. \;
                Replace the current set of weighted particles with
                $(\theta^m, W^m) \gets (\tilde{\theta}^m, 1 / M)$. \;
              }
              \KwRet{$\{\theta^m, W^m\}_{m=1}^M$.} \;
            }
          \end{algorithm}
        }
        \end{column}
      \end{columns}
    \end{frame}

    \begin{frame}{Inside-Out \texorpdfstring{SMC\textsuperscript{2}}{SMC2}}
      \begin{itemize}
        \item Assume that at time $t$ we have a trajectory $z_{0:t}^n$, where $n = 1, \dots, N$.
        \item Assume that we also have a particle approximation of the filtering posterior of $\theta$ obtained using IBIS:
          \begin{equation}
            p(\theta \mid z_{0:t}^n) \approx \hat{p}(\theta \mid z_{0:t}^n) \coloneq \sum_{m=1}^M W_{t,\theta}^{mn} \, \delta_{\theta_t^{mn}}(\theta).
          \end{equation}
          where $\delta$ is the Dirac delta function.
        \item We can then form an approximation to the marginal dynamics for the state as
          \begin{align}
            \hat{p}(x_{t+1} \mid z_{0:t}^n, \xi_t) &= \int f(x_{t+1} \mid x_t^n, \xi_t, \theta) \, \hat{p}(\theta \mid z_{0:t}^n) \, \dd\theta \\
            &= \sum_{m=1}^M W_{t,\theta}^{mn} \, f(x_{t+1} \mid x_t^n, \xi_t, \theta_t^{mn}).
      \end{align}
      \end{itemize}
    \end{frame}

    \begin{frame}{Inside-Out \texorpdfstring{SMC\textsuperscript{2}}{SMC2}}
      \begin{algorithm}[H]
        \setstretch{1.35}
        \SetAlgoLined
        \DontPrintSemicolon
        \LinesNumbered
        Sample $z_0^n \sim p(z_0)$, $\theta_0^{mn} \sim p(\theta)$ and set $W_{0, \theta}^{mn} \gets 1/M$. \;
        Sample $z_1^n \sim \hat{p}_\phi(\cdot \mid z_0^n)$ and initialize the state history $z_{0:1}^n \gets (z_0^n, z_1^n)$. \;
        Compute and normalize the weights
            \vspace{-0.2cm} $$\smash{W_z^n \propto \exp \Big \{-\eta \log \hat{p}(x_1^n \mid x_0^n, \xi_0^n) \Big\}.}$$ \vspace{-0.6cm}\;
        \For{$t \gets 1, \dots, T - 1$}{
          Sample $b_t^n \sim \mathcal{M}(W_z^{1:N})$. \;
          $\theta_t^{\bullet n}, W^{\bullet n}_{t, \theta}, \gets \textsc{Ibis\_Step}(z_{0:t}^{b_t^n}, \theta_{t-1}^{\bullet b_t^n}, W^{\bullet b_t^n}_{t - 1, \theta})$ \;
          Sample $z_{t+1}^n \sim \hat{p}_\phi(\cdot \mid z_{0:t}^{b_t^n}),$
              and append to state history $z_{0:t+1}^n \gets [z_{0:t}^{b_t^n}, z_{t+1}^n]$. \;
          Compute and normalize the weights
              \vspace{-0.2cm} $$\smash{W_z^n \propto \exp\left\{-\eta \log \hat{p}(x_{t+1}^n \mid z_{0:t}^{b_t^n}, \xi_t^n)\right\}.}$$ \vspace{-0.6cm} \;
        }
        \KwRet{$\{z_{0:T}^n, W_z^n\}_{n=1}^N$.}
        \label{alg:iosmc}
      \end{algorithm}
    \end{frame}

    \begin{frame}{Inside-Out \texorpdfstring{SMC\textsuperscript{2}}{SMC2}}
      \begin{itemize}
        \item The algorithm asymptotically targets the distribution $\Gamma_\phi(z_{0:T})$~(Proposition 2).
        \item A conditional version of the algorithm can be developed in the same way as for state-space models~(Appendix C.4).
      \end{itemize}
    \end{frame}

    \begin{frame}{Preliminary Empirical Results}
      \begin{itemize}
        \item We consider the conditionally linear formulation of the compound pendulum as given in~\citet{belusov2019belief}.
        \item The unknown parameters are $\theta = \displaystyle \left(\frac{3g}{2l}, \frac{3d}{ml^2}, \frac{3}{ml^2} \right)$.
        \item The dynamics is described by the following Ito SDE
          \begin{equation}
            dx_t = h(x_t, \xi_t)^T \, \theta \, \dd t + L \, \dd \beta,
          \end{equation}
          %
          where $h(x_t, \xi_t) = (-\sin(q), -\dot{q}, \xi_t)^T$, $L = (0, 0.1)^T$ and $\beta$ denotes Brownian motion.
      \end{itemize}
    \end{frame}

    \begin{frame}{Preliminary Empirical Results}
      \begin{figure}[t]
        \input{figures/pendulum_info_gain}
        \vspace{-0.25cm}
        \caption{Information gain of different policies computed in closed-form on the conditionally-linear stochastic pendulum. We report the mean and standard deviation over 512 realizations.}
        \label{fig:pendulum_info_gain}
      \end{figure}
    \end{frame}

    \begin{frame}{Preliminary Empirical Results}
      \begin{figure}[!h]
        \centering
        \input{figures/pendulum_sample_trajectory}
        \vspace{-0.25cm}
        \caption{A sample experiment trajectory generated by the amortized policy during deployment on the non-linear stochastic pendulum environment. $q$ is the angle of the pendulum from the vertical, $\dot{q}$ is the angular velocity and $\xi$ is the design.}
        \label{fig:pendulum_sample_trajectory}
      \end{figure}
    \end{frame}

  % Bibliography
  \begin{frame}[t, allowframebreaks]{References}
    \bibliography{references}
  \end{frame}

\end{document}
